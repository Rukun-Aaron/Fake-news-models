{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  "
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,  pipeline\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
>>>>>>> d28f09e938ddb87c5b9cf1317e949c6e34a6d609
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
=======
    "tweet = \"Indonesian police have recaptured a U.S. citizen who escaped a week ago from an overcrowded prison on the holiday island of Bali, the jail s second breakout of foreign inmates this year.  Cristian Beasley from California was rearrested on Sunday, Badung Police chief Yudith Satria Hananta said, without providing further details.  Beasley was a suspect in crimes related to narcotics but had not been sentenced when he escaped from Kerobokan prison in Bali last week. The 32-year-old is believed to have cut through bars in the ceiling of his cell before scaling a perimeter wall of the prison in an area being refurbished. The Kerobokan prison, about 10 km (six miles) from the main tourist beaches in the Kuta area, often holds foreigners facing drug-related charges. Representatives of Beasley could not immediately be reached for comment. In June, an Australian, a Bulgarian, an Indian and a Malaysian tunneled to freedom about 12 meters (13 yards) under Kerobokan prison s walls. The Indian and the Bulgarian were caught soon after in neighboring East Timor, but Australian Shaun Edward Davidson and Malaysian Tee Kok King remain at large. Davidson has taunted authorities by saying he was enjoying life in various parts of the world, in purported posts on Facebook.  Kerobokan has housed a number of well-known foreign drug convicts, including Australian Schappelle Corby, whose 12-1/2-year sentence for marijuana smuggling got huge media attention.\"\n",
    "\n",
    "roberta = 'jy46604790/Fake-News-Bert-Detect'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake: 0.05004359991289675%\n",
      "Real: 99.94995594024658%\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "labels = ['Fake', 'Real']\n",
    "encoded = tokenizer(tweet, return_tensors=\"pt\")\n",
    "\n",
    "output = model(**encoded)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "for i in range (0, len(labels)):\n",
    "  print(f\"{labels[i]}: {(scores[i] * 100)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> d28f09e938ddb87c5b9cf1317e949c6e34a6d609
    "\n",
    "train_path = 'LIAR/train.tsv'\n",
    "train_data = pd.read_csv(train_path,sep='\\t', header=None, names=[\"id\", \"label\", \"statement\", \"subject(s)\", \"speaker\",\"speaker's job title\", \"state info\", \"party affiliation\", \"barely true counts\", \"false counts\",\"half true counts\", \"mostly true counts\", \"pants on fire counts\", \"context\"])\n",
    "\n",
    "train_data['label'] = train_data['label'].replace(['pants-fire', 'barely-true','false'], 0)\n",
    "train_data['label'] = train_data['label'].replace(['half-true', 'mostly-true','true'], 1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# import nltk \n",
    "# nltk.download('vader_lexicon')\n",
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\rukun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk \n",
    "nltk.download('vader_lexicon')\n",
>>>>>>> d28f09e938ddb87c5b9cf1317e949c6e34a6d609
    "sid = SentimentIntensityAnalyzer()\n",
    "def analyze_sentiment(text):\n",
    "    if isinstance(text, str) == False and text != 'NaN':\n",
    "        return None\n",
    "    words = word_tokenize(text)\n",
    "   \n",
    "    # filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(words)\n",
    "\n",
    "    sentiment_scores = sid.polarity_scores(filtered_text) \n",
    "    return sentiment_scores['compound'] +1"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rukun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rukun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
>>>>>>> d28f09e938ddb87c5b9cf1317e949c6e34a6d609
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2500</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3612</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3182</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.7579</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                          statement\n",
       "0     1.2500  Says the Annies List political group supports ...\n",
       "1     1.3612  When did the decline of coal start? It started...\n",
       "2     1.3182  Hillary Clinton agrees with John McCain \"by vo...\n",
       "3     1.7579  Health care reform legislation is likely to ma...\n",
       "4     1.0000  The economic turnaround started at the end of ..."
      ]
     },
<<<<<<< HEAD
     "execution_count": 5,
=======
     "execution_count": 8,
>>>>>>> d28f09e938ddb87c5b9cf1317e949c6e34a6d609
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
<<<<<<< HEAD
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
=======
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
>>>>>>> d28f09e938ddb87c5b9cf1317e949c6e34a6d609
    "stop_words = set(stopwords.words('english'))\n",
    "train_data['sentiment'] = train_data['statement'].apply(analyze_sentiment)\n",
    "train_features = train_data[['sentiment', 'statement']]\n",
    "y_train = train_data['label']\n",
<<<<<<< HEAD
    "train_features.head()\n"
=======
    "train_features.head()"
>>>>>>> d28f09e938ddb87c5b9cf1317e949c6e34a6d609
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification,  pipeline\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m softmax\n\u001b[0;32m      3\u001b[0m roberta_link \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mjy46604790/Fake-News-Bert-Detect\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_link = 'jy46604790/Fake-News-Bert-Detect'\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= []\n",
    "for statement in train_features['statement'][:1000]:\n",
    "    # encoded_statement = new_tokenizer(statement,return_tensors=\"pt\")\n",
    "    encoded_statement = new_tokenizer.encode(statement, padding=True, truncation=True,max_length=50, add_special_tokens = True,return_tensors=\"pt\")\n",
    "    output = model(encoded_statement)\n",
    "    # print(encoded_statement)\n",
    "    # print(len(encoded_statement['input_ids'][0]), len(encoded_statement['attention_mask'][0]))\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    # for i in range (0, len(labels)):\n",
    "    if scores[0]>= scores[1]:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.439\n"
>>>>>>> d28f09e938ddb87c5b9cf1317e949c6e34a6d609
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,  pipeline\n",
    "from scipy.special import softmax\n",
    "roberta_link = 'jy46604790/Fake-News-Bert-Detect'\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_link)\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(roberta_link)"
=======
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_train[:1000], predictions)\n",
    "print(accuracy)"
>>>>>>> d28f09e938ddb87c5b9cf1317e949c6e34a6d609
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "for statement in train_features['statement'][:10]:\n",
    "    encoded_statement = new_tokenizer(statement,return_tensors=\"pt\")\n",
    "    output = model(**encoded_statement)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    for i in range (0, len(labels)):\n",
    "        print(f\"{labels[i]}: {(scores[i] * 100)}%\")"
   ]
=======
   "source": []
>>>>>>> d28f09e938ddb87c5b9cf1317e949c6e34a6d609
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.11"
=======
   "version": "3.10.5"
>>>>>>> d28f09e938ddb87c5b9cf1317e949c6e34a6d609
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
