{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_path = 'LIAR/train.tsv'\n",
    "train_data = pd.read_csv(train_path,sep='\\t', header=None, names=[\"id\", \"label\", \"statement\", \"subject(s)\", \"speaker\",\"speaker's job title\", \"state info\", \"party affiliation\", \"barely true counts\", \"false counts\",\"half true counts\", \"mostly true counts\", \"pants on fire counts\", \"context\"])\n",
    "\n",
    "train_data['label'] = train_data['label'].replace(['pants-fire', 'barely-true','false'], 0)\n",
    "train_data['label'] = train_data['label'].replace(['half-true', 'mostly-true','true'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# import nltk \n",
    "# nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def analyze_sentiment(text):\n",
    "    if isinstance(text, str) == False and text != 'NaN':\n",
    "        return None\n",
    "    words = word_tokenize(text)\n",
    "   \n",
    "    # filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(words)\n",
    "\n",
    "    sentiment_scores = sid.polarity_scores(filtered_text) \n",
    "    return sentiment_scores['compound'] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.2500</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3612</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3182</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.7579</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                          statement\n",
       "0     1.2500  Says the Annies List political group supports ...\n",
       "1     1.3612  When did the decline of coal start? It started...\n",
       "2     1.3182  Hillary Clinton agrees with John McCain \"by vo...\n",
       "3     1.7579  Health care reform legislation is likely to ma...\n",
       "4     1.0000  The economic turnaround started at the end of ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train_data['sentiment'] = train_data['statement'].apply(analyze_sentiment)\n",
    "train_features = train_data[['sentiment', 'statement']]\n",
    "y_train = train_data['label']\n",
    "train_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification,  pipeline\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m softmax\n\u001b[0;32m      3\u001b[0m roberta_link \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mjy46604790/Fake-News-Bert-Detect\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,  pipeline\n",
    "from scipy.special import softmax\n",
    "roberta_link = 'jy46604790/Fake-News-Bert-Detect'\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_link)\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(roberta_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for statement in train_features['statement'][:10]:\n",
    "    encoded_statement = new_tokenizer(statement,return_tensors=\"pt\")\n",
    "    output = model(**encoded_statement)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    for i in range (0, len(labels)):\n",
    "        print(f\"{labels[i]}: {(scores[i] * 100)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
